---
layout: post
author: djeongo
---
LMS(Least Mean Squares) filter is a type of adaptive filter.  
This post reviews Chapter 13. Adaptive Filters section in [1].  
I try to expand out the equations in more detail for better understanding.

We use estimates of the autocorrelation and crosscorrelation to determine   
the filter coefficients. The derivation below shows why autocorrelation and   
craosscorrelation are needed.

## Estimated error

In adaptive filtering, we want to minimize the following estimation error:

$$ \begin{align*} e(n) &= d(n) - \hat{d}(n)  \\
        &= d(n) - \sum_{k=0}^{M-1}{h(k)x(n-k})
\end{align*}
$$

where $$ d(n) $$ is the desired signal,  
$$ \hat{d}(n) $$ is the estimated signal,  
and $$ h(k) $$ is the filter coefficients.

## Mean-square of error
The mean-square of the error as a function of the filter coefficients $$ h(n) $$:

$$ \begin{align*}

\mathscr{E}_M &=E[|e(n)|^2] \\
&=E[(d(n) - \hat{d}(n))] \\
&=E[|d(n) - \sum_{k=0}^{M-1}{h(k)x(n-k})|^2] \\
&=E[(d(n) - \sum_{k=0}^{M-1}{h(k)x(n-k}))(d(n) - \sum_{l=0}^{M-1}{h(l)x(n-l}))^{*}] \\
&=E[|d(n)|^2 - \sum_{k=0}^{M-1}{h(k)x(n-k})d^{*}(n) - \sum_{l=0}^{M-1}{h^{*}(l)x^{*}(n-l})d(n) + \sum_{k=0}^{M-1}\sum_{l=0}^{M-1}{h(k)h^{*}(l)x(n-k)x^{*}(n-l})]\\
&=E\big[|d(n)|^2\big] - E\Big[\sum_{k=0}^{M-1}{h(k)x(n-k})d^{*}(n)\Big] - E\Big[\sum_{l=0}^{M-1}{h^{*}(l)x^{*}(n-l})d(n)\Big] + E\Big[\sum_{k=0}^{M-1}\sum_{l=0}^{M-1}{h(k)h^{*}(l)x(n-k)x^{*}(n-l})\Big] \\
&=\sigma^{2}_{d} - \sum_{k=0}^{M-1}{h(k)E\Big[x(n-k})d^{*}(n)\Big] - \sum_{l=0}^{M-1}{h^{*}(l)E\Big[x^{*}(n-l})d(n)\Big] + \sum_{k=0}^{M-1}\sum_{l=0}^{M-1}{h(k)h^{*}(l)E\Big[x(n-k)x^{*}(n-l})\Big]\\
&=\sigma^{2}_{d} - \sum_{k=0}^{M-1}{h(k)\gamma_{dx}(k)} - \sum_{l=0}^{M-1}{h^{*}(l)\gamma_{dx}(l)} + \sum_{k=0}^{M-1}\sum_{l=0}^{M-1}{h(k)h^{*}(l)\gamma_{xx}(l-k)}\\
\end {align*} $$

### Minimize mean-square error

#### Take derivative

Taking the derivative of $$ \mathscr{E}_M $$ with respect to $$ h^{*}(l) $$ gives:

$$ \begin{align*}
\frac{\partial \mathscr{E}_M}{\partial h^{*}(l)} &= -\gamma_{dx}(l) + \sum_{k=0}^{M-1}h(k)\gamma_{xx}(l-k) \\
\end{align*} $$

Setting the derivative to zero yields:

$$ \gamma_{dx}(l) = \sum_{k=0}^{M-1}h(k)\gamma_{xx}(l-k), \qquad l = 0,1,...,M-1 $$

These equations are also called the *Wiener-Hopf equation* or the *normal equations*.

#### Alternative: orthogonality principal

The orthogonality principal states that **the mean-square estimation error is minimized  
when the error is orthogonal, in the statistical sense, to the estimate $$ \hat{d}(n) $$**.

$$\begin{align*}

 E[e(n)\hat{d^*}(n)] &= 0  \\
 E[e(n)\sum_{k=0}^{M-1}h^{*}(k)x^{*}(n-k)] &= 0 \\
 E[\sum_{k=0}^{M-1}h^{*}(k)e(n)x^{*}(n-k)] &= 0 \\
 \sum_{k=0}^{M-1}h^{*}(k)E[e(n)x^{*}(n-k)] &= 0 \\
\end{align*}$$ 

which implies that the error vector is orthogonal to each of the data points

$$ E[e(n)x^{*}(n-l)] = 0, \qquad l = 0,1,...M-1 $$

Substitute  $$ e(n) $$ and taking expectation yields the same set of normal equations.

$$\begin{align*}
 E[(d(n) - \sum_{k=0}^{M-1}h(k)x(n-k))x^{*}(n-l)] &= 0  \\
 &= E[d(n)x^{*}(n-l) - \sum_{k=0}^{M-1}h(k)x(n-k)x^{*}(n-l)]  \\
 &= E[d(n)x^{*}(n-l)] - E[\sum_{k=0}^{M-1}h(k)x(n-k)x^{*}(n-l)] \\
 &= \gamma_{dx}(l) - \sum_{k=0}^{M-1}h(k)E[x(n-k)x^{*}(n-l)]   \\
 &= \gamma_{dx}(l) - \sum_{k=0}^{M-1}h(k)\gamma_{xx}(l-k)  \\
 &= \mathbf{\Gamma}_{M}\textbf{h}_M(n) - \mathbf{\gamma}(d) = 0 
\end{align*}$$

## Estimating the filter coefficients
In general, we can recursively estimate the filter coefficients in the following way.

$$ \textbf{h}_{M}(n+1) = \textbf{h}_{M}(n) + \frac{1}{2}\Delta \textbf{S}(n) $$

where,
$$ \textbf{h}_M $$ denotes the coefficient vector,  
$$ \delta $$ denotes the step size,  
$$ \textbf{S}(n) $$ denotes the direction vector,
and $$ n $$ is the current iteration.

There are different ways to compute the direction vector $$ \textbf{S}(n) $$.

### With $$ \gamma_{dx} $$ and $$ \gamma_{xx} $$ known
The following methods can be used if$$ \gamma_{dx} $$ and $$ \gamma_{xx} $$ are known.
#### Steepest-descent search

$$ \textbf{S}(n) = -\textbf{g}(n) $$

where 

$$ \begin{align*}
\textbf{g}(n) &=\frac{\partial \mathscr{E}_M}{\partial \textbf{h}_{M}} \\
              &= 2[\mathbf{\Gamma}_{M}\textbf{h}_M(n) - \mathbf{\gamma}(d)]
\end{align*}$$

#### Conjugate-gradient algorithm

$$\textbf{S}(n) = \beta(n-1)\textbf{S}(n-1) - \textbf{g}(n-1) $$

#### Fletcher-Powell algorithm

$$\textbf{S}(n) = -\textbf{H}(n)g(n) $$

### With $$ \gamma_{dx} $$ and $$ \gamma_{xx} $$ unknown
When the autocorrelation and crosscorrelation values are unknown, we use the orthogonality principal to note that 

$$ E[e(n)\mathbf{X}^{*}_M]  = \mathbf{\Gamma}_{M}\textbf{h}_M(n) - \mathbf{\gamma}(d) $$

where $$ \mathbf{X}^{*}_M $$ is a vector with elements $$ x(n-l), l=0,1,...M-1 $$
so we set $$ \mathbf{g}(n) = -2E[e(n)\mathbf{X}^{*}_M] $$ which doesn't require knowledge of   autocorrelation or crosscorrelation.

We can use *stochastic-gradient-descent algorithm* by using the estimate,

$$ \mathbf{\hat{g}}(n) = -2e(n)\mathbf{X}^{*}_M $$

Finally the LMS algorithm is,

$$ \textbf{h}_{M}(n+1) = \textbf{h}_{M}(n) + \Delta e(n)\mathbf{X}^{*}_M $$


[1] Digital Signal Processing Principles, Algorithms, and Applications - John G. Proakis, Dimitris G. Manolakis
